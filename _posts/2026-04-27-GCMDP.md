---
layout: distill
title: Reward maximization using Goal-conditioned RL
description: 

date: 2026-04-27
future: true
htmlwidgets: true
hidden: true

# Mermaid diagrams
mermaid:
  enabled: true
  zoomable: true

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Anonymous
    url: 
    affiliations:
      name: 


# must be the exact same name as your blogpost
bibliography: 2026-04-27-GCMDP.bib

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction 
  - name: Problem Formulation
  - name: Pratical Algorithm
  - name: Experiment


# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---
# Introduction


Goal-conditioned reinforcement learning (GCRL) is a framework where agents learn to reach specified goal states <d-cite key="eysenbach2022contrastive"></d-cite>. While the sparse reward structure of this task might seem to hinder exploration, recent work has demonstrated a surprising finding: contrastive RL algorithms that train with only a single challenging downstream goal‚Äîrather than a distribution of goals or using any intrinsic shaped reward‚Äîcan effectively explore the environment and acquire useful skills without any supervision <d-cite key="liu2025a,bastankhah2025"></d-cite>. This raises a natural question: can we leverage the exploration advantages of GCRL to solve standard reward maximization problems?

In this work, we provide a positive answer by showing how any reward maximization MDP can be converted into an equivalent goal-conditioned MDP. Our construction augments the original MDP with synthetic absorbing states, where reaching the "success" absorbing state corresponds exactly to maximizing the expected return in the original problem. This equivalence enables practitioners to apply goal-conditioned RL algorithms to traditional reward maximization tasks, potentially benefiting from the superior exploration properties of GCRL methods.


# Problem Formulation



We consider a Markov Decision Process (MDP) defined as the tuple
$$
\mathcal{M} = (\mathcal{S}, \mathcal{A}, p, r, \rho_0, \gamma),
$$
where $$\mathcal{S}$$ is the state space, $$\mathcal{A}$$ is the action space, $$p(s'|s,a)$$ is the transition kernel, $$r(s)\in[0,1]$$ is the state-only reward function, $$\rho_0$$ is the initial state distribution, and $$\gamma\in(0,1)$$ is the discount factor.  
The expected discounted return under a policy $$\pi(a|s)$$ is:

$$
J(\pi) = \mathbb{E}_{\pi, \rho_0}\left[\sum_{t=0}^\infty \gamma^{\,t} r(s_t) \mid s_0\right].
$$


The discounted state occupancy measure of policy $$\pi$$ is:

$$
d^\pi_\gamma(s) 
:= (1-\gamma)
\mathbb{E}_{\rho_0}\left[\sum_{t=0}^\infty \gamma^{t} 
p^{\pi}(s_t = s \mid s_0)\right].
$$

This satisfies two properties: $$d^\pi_\gamma$$ is a normalized distribution over $$\mathcal{S}$$, and  
  $$
  \mathbb{E}_{d^\pi_\gamma(s)}[r(s)]
  = (1-\gamma)\,
  \mathbb{E}_{\pi, \rho_0}\Big[\sum_{t\ge 0}\gamma^{t} r(s_t) \mid s_0\Big]
  $$

Thus maximizing return is equivalent to maximizing the expected reward under the discounted occupancy measure.





A goal-conditioned MDP (GC-MDP) is a special case of an MDP where the reward function takes the form $$r(s,g) = \mathbb{1}[s=g]$$, indicating whether the agent has reached a specified goal state $$g$$. For such a GC-MDP, the expected discounted return can be expressed as:

$$
J(\pi) = \mathbb{E}_{\pi, \rho_0}\left[\sum_{t=0}^\infty \gamma^{\,t} r(s_t,g) \mid s_0\right] = d^\pi_\gamma(g),
$$

which shows that maximizing the return is equivalent to maximizing the goal-reaching probability under the discounted occupancy measure. Our key insight is to construct a GC-MDP where the task of reaching a synthetic absorbing state $$s^+$$ becomes equivalent to maximizing the expected return in the original MDP. This reformulation allows us to leverage goal-conditioned reinforcement learning algorithms to solve standard return-maximization problems.



## Constructing the Goal-Conditioned MDP

We construct a new GC-MDP $$\widetilde{\mathcal{M}} = (\widetilde{\mathcal{S}}, \mathcal{A}, \tilde{p}, \tilde{r}, \rho_0, \tilde{\gamma})$$ such that maximizing the probability of reaching a single absorbing state $$s^+$$ corresponds exactly to maximizing expected return in the original MDP. The state space augments the original MDP with two additional absorbing states:

$$
\widetilde{\mathcal{S}} = \mathcal{S} \cup \{s^+, s^-\},
$$

where $$s^+$$ represents a successful terminal state and $$s^-$$ represents a failure terminal state. Both states are absorbing, meaning that once the agent reaches either state, it remains there indefinitely.

For all $$s \in \mathcal{S}$$:

$$
\tilde{p}(s' \mid s,a)
=
\begin{cases}
s^+, &\text{with probability } (1-\alpha)\, r(s) \\
s^-, &\text{with probability } (1-\alpha)\, (1-r(s)) \\
p(s'|s,a), &\text{with probability } \alpha
\end{cases}
$$

Where $$\alpha \in (0, 1)$$
Absorbing states satisfy:

$$
\tilde{p}(s'|s^+,a)=\mathbf{1}(s'=s^+), \\
\tilde{p}(s'|s^-,a)=\mathbf{1}(s'=s^-).
$$



At each time step, with probability $$\alpha$$ the agent follows the original dynamics, and with probability $$1-\alpha$$ the episode terminates by jumping to either $$s^+$$ or $$s^-$$.




## Relating The goal reaching probability to the reward maximization Return

Let $$t_{s^+}$$ denote the first time the agent enters $$s^+$$. For $$k\ge 1$$:

$$
\begin{align*}
\tilde{p}^{\pi}(t_{s^+} = k) &= \sum_{s' \in \mathcal{S}} \tilde{p}^{\pi}(s_{k-1}=s') \,\tilde{p}(s^+|s') \\
&=  \sum_{s' \in \mathcal{S}} \tilde{p}^{\pi}(s_{k-1}=s')\,(1-\alpha) \,r(s') \\
&= \sum_{s' \in \mathcal{S}} \alpha^{k-1} p^{\pi}(s_{k-1}=s')\,(1-\alpha) \,r(s')
\end{align*}
$$

The last equality follows because for the agent to be in a non-absorbing state $$s'$$ at time $$k-1$$ in the GC-MDP, it must have avoided transitioning to any absorbing state in all previous timesteps, which occurs with probability $$\alpha^{k-1}$$, while following the dynamics of the original MDP to reach $$s'$$.


The $$\tilde{\gamma}$$-discounted occupancy of $$s^+$$ is:


$$
\begin{align*}
\tilde{d}^\pi_{\tilde{\gamma}}(s^+ \mid s_0) &=(1-\tilde{\gamma})\sum_{t\geq 0} \tilde{\gamma}^t\tilde{p}^\pi(s_t=s^+ \mid s_0)\\
&=(1-\tilde{\gamma})\sum_{t\geq 0} \tilde{\gamma}^t\tilde{p}^\pi(t_{s^+}\leq t \mid s_0)\\
&=(1-\tilde{\gamma})\sum_{t\geq 0} \tilde{\gamma}^t\sum_{k \leq t}\tilde{p}^\pi(t_{s^+}= k \mid s_0)\\
&= (1-\tilde{\gamma}) \sum_{k \geq 1} \tilde{p}^{\pi}(t_{s^+} = k\mid s_0) \sum_{t\geq k}\tilde{\gamma}^t \\
&=\sum_{k \geq 1} \tilde{\gamma}^k \tilde{p}^{\pi}(t_{s^+} = k\mid s_0) \\
&=  (1-\alpha) \,\tilde{\gamma}\, \sum_{k \geq 1} \tilde{\gamma}^{k-1}  \sum_{s' \in \mathcal{S}} \alpha^{k-1} p^{\pi}(s_{k-1}=s'\mid s_0)\,r(s')\\
&= (1-\alpha) \,\tilde{\gamma}\, \sum_{s' \in \mathcal{S}} r(s') \sum_{k \geq 0} (\tilde{\gamma}\alpha)^{k} \,p^{\pi}(s_{k}=s'\mid s_0)\\
&= \frac{(1-\alpha) \,\tilde{\gamma} }{1-\alpha \tilde{\gamma}}\,\mathbb{E}_{d^\pi_\gamma(s\mid s_0)}[r(s)]
\end{align*}
$$

Recognizing that the term $$\sum_{k\geq 0} (\alpha\tilde{\gamma})^k p^\pi(s_k=s')$$ represents the occupancy measure of the original MDP with an effective discount factor of $$\gamma = \alpha \tilde{\gamma}$$, we obtain the final result. Thus, maximizing the original return with discount factor $$\gamma$$ is equivalent to maximizing $$\tilde{d}^\pi_{\tilde{\gamma}}(s^+)$$ in the constructed GC-MDP, where:

Recognizing that the term $$\sum_{k\geq 0} (\alpha\tilde{\gamma})^k p^\pi(s_k=s')$$ represents the occupancy measure of the original MDP with an effective discount factor of $$\gamma = \alpha \tilde{\gamma}$$, we obtain the final result. Thus, maximizing the original return with discount factor $$\gamma$$ is equivalent to maximizing $$\tilde{d}^\pi(s^+)$$ in the constructed GC-MDP, where:

$$
\gamma = \alpha\,\tilde{\gamma}.
$$

Hence, we have established the equivalence:

$$
J(\pi) = d^\pi_{\tilde{\gamma}}(s^+)
$$

where $$J(\pi)$$ is the reward maximization objective in the original MDP.

---

<div style="background-color: #E3F2FD; padding: 15px; border-left: 4px solid #1E88E5; margin: 20px 0;">
<span style="color: #1565C0; font-weight: 600;">üìù Note on Extensions:</span>
<ul style="color: #424242; margin-top: 8px;">
<li><strong>Non-normalized rewards:</strong> For rewards $$r(s) \in [r_{\min}, r_{\max}]$$, apply normalization: $$\hat{r}(s) = \frac{r(s) - r_{\min}}{r_{\max} - r_{\min}}$$ to transform to $$[0,1]$$.</li>
<li><strong>State-action rewards:</strong> For $$r(s,a)$$, marginalize over actions: $$\hat{r}(s) = \mathbb{E}_{a \sim \pi(\cdot|s)}[r(s,a)]$$ to convert to state-only rewards.</li>
</ul>
</div>

---

# Pratical Algorithm

# Experiments

