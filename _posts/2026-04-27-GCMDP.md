---
layout: distill
title: Reward maximization using Goal-conditioned RL
description: |
  Goal-conditioned reinforcement learning (GCRL) has recently demonstrated
  remarkable success as an unsupervised framework capable of driving strong
  exploratory behavior. Because reaching a goal from scratch is often harder
  than maximizing a dense reward, algorithms that succeed in GCRL implicitly
  solve a challenging exploration problem. This raises a natural question:
  can advances in GCRL be leveraged to solve sparse-reward return-
  maximization problems? In this work, we answer this question affirmatively.
  We show how to construct, from any reward-maximization MDP, an equivalent
  goal-conditioned MDP in which the probability of reaching a synthetic
  absorbing goal state matches the return of the original task.
  
  Consequently, running any goal-conditioned RL algorithm on this transformed MDP yields a policy that maximizes return in the original environment. We demonstrate the effectiveness of this reduction on Riverswim, a canonical hard-exploration benchmark where the agent receives a small reward near the start and a significantly larger reward only after persistent exploration. As we increase the difficulty of the exploration problem (by lengthening the river), our method‚Äôs advantage over standard reward-maximization baselines grows substantially, highlighting the practical value of leveraging GCRL for challenging sparse-reward domains.


date: 2026-11-27
future: true
htmlwidgets: true
hidden: true

# Mermaid diagrams
mermaid:
  enabled: true
  zoomable: true

# Anonymize when submitting
authors:
  - name: Anonymous

# authors:
#   - name: Anonymous
#     url: 
#     affiliations:
#       name: 


# must be the exact same name as your blogpost
bibliography: 2026-04-27-GCMDP.bib

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction 
  - name: Problem Formulation
  - name: Pratical Algorithm
  - name: Experiment


# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  .detail { color: #1f77b4; cursor: pointer; display: inline-block; margin-bottom: 1rem; }
---
# Introduction


<!-- Many real-world decision-making problems, from robotic manipulation and autonomous driving to traffic
routing and healthcare operations, have access to large amounts of unlabeld prior data and require agents
to reason over hundreds or thousands of steps. Standard reinforcement learning (RL) methods struggle in
this setting, as they heavily depend on hand-crafted reward functions and extensive task-specific tuning,
which can lead to brittle behavior and reward hacking, where agents exploit edge cases in the reward at
the expense of safety or robustness. -->

Reinforcement learning (RL) has achieved great success in solving many decision-making problems, from outperforming human-level control on challenging games<d-cite key="silver2018general"></d-cite> and fine-tuning large language models<d-cite key="shao2024deepseekmath"></d-cite> to learning scalable robotic manipulation policies<d-cite key="amin2025pi"></d-cite> Goal-conditioned reinforcement learning (GCRL) is an important type of reinforcement learning (RL) problems, which prevents  hand-crafted reward functions. Instead, GCRL algorithms learn to reach different goals specified by the environment<d-cite key="kaelbling1993learning,schaul2015universal"></d-cite>.
<!-- Goal-conditioned reinforcement learning (GCRL) is a framework where agents learn to reach specified goal states <d-cite key="eysenbach2022contrastive"></d-cite>.  -->
While the sparse reward structure of this task might seem to hinder exploration, recent work has demonstrated a surprising finding: contrastive RL algorithms that train with only a single challenging downstream goal‚Äîrather than a distribution of goals or using any intrinsic shaped reward‚Äîcan effectively explore the environment and acquire useful skills without any supervision <d-cite key="liu2025a,bastankhah2025"></d-cite>. This raises a natural question: can we leverage the exploration advantages of GCRL to solve standard reward maximization problems?





In this work, we provide a positive answer by showing how any reward maximization MDP can be converted into an equivalent goal-conditioned MDP. Our construction augments the original MDP with synthetic absorbing states, where reaching the "success" absorbing state corresponds exactly to maximizing the expected return in the original problem. This equivalence enables practitioners to apply goal-conditioned RL algorithms to traditional reward maximization tasks, potentially benefiting from the superior exploration properties of GCRL methods.

<span style="color:red; font-weight:bold">TODO: @Chongyi motivate the method by arguing that goal conditioned RL might be easier to solve by benefiting from Quasimetric</span>

# Related Work
Goal-conditioned reinforcement learning (GCRL) focuses on learning policies that can reach specified goal states. A key development was Universal Value Function Approximators (UVFA), which parameterized value functions by goals and enabled generalization across both states and goals <d-cite key="pmlr-v37-schaul15"></d-cite>. This was followed by Hindsight Experience Replay (HER), which improved sample efficiency by relabeling failed trajectories with achieved goals <d-cite key="NIPS2017_453fadbd"></d-cite>. Another successful method is contrastive RL, which learns representations that capture goal-reaching probabilities by bringing state‚Äìaction embeddings closer to embeddings of their future states <d-cite key="eysenbach2022contrastive"></d-cite>.


Exploration is difficult in goal-conditioned settings because rewards are sparse and the agent must discover long action sequences to reach the goal. Prior work has addressed this challenge through intrinsic reward that encourage novelty or curiosity <d-cite key="pathak2017curiosity"></d-cite>, as well as hierarchical RL approaches that learn subgoal structures to guide long-horizon exploration <d-cite key="10.5555/3327144.3327250,levy2019learningmultilevelhierarchieshindsight"></d-cite>. More recent work has shown that contrastive-learning‚Äìbased approaches can exhibit strong exploration without any curriculumn or intrinsic reward, revealing surprising emergent exploration behaviour <d-cite key="eysenbach2022contrastive,liu2025a,bastankhah2025"></d-cite>.



Converting a reward maximization MDP to a goal reaching MDP has been studied in <d-cite key="bertsekasNDP"></d-cite> however the focus of our work is on improving exploration in sparse reward maximziation setting by using the advances of GCRL. <span style="color:red; font-weight:bold">TODO: @Chongyi can you pls elaborate on this?</span>


<span style="color:red; font-weight:bold">TODO: @Chongyi can you pls add the literature review about Quasi metric?</span>
# Problem formulation

<strong>Markov decision process and occupancy measures.</strong> We consider a Markov decision process (MDP) defined by a state space $\mathcal{S}$, an action space $\mathcal{A}$, a transition distribution $p: \mathcal{S} \times \mathcal{A} \to \Delta(\mathcal{S})$, a reward function $r: \mathcal{S} \to [0, 1]$, an initial state distribution $p_0 \in \Delta(\mathcal{S})$, and a discount factor $\gamma \in [0, 1)$, where $\Delta(\cdot)$ denotes the set of all possible probability distributions over a space. We use $$t$$ to denote the time step in MDP and assume the reward function is normalized and only depends on the state without loss of generality<d-cite key="thakoor2022generalised,ghosh2023reinforcement,frans2024unsupervised"></d-cite>.

<!-- <details style="background-color: #f4f4f4ff; padding: 15px; border-left: 4px solid #1E88E5; margin: 20px 0;">
  <summary>Discussion about the reward function</summary>
  For a reward function $\hat{r}(s)$ taking values in $[r_{\min}, r_{\max}]$, we can apply the min-max normalization to transform the range into $[0, 1]$:
  $$
  r(s) = \frac{\hat{r}(s) - r_{\min}}{r_{\max} - r_{\min}}.
  $$

  For a reward function depending on the state-action pair $\hat{r}(s, a)$, we can marginalize over actions using the policy $\pi$ to convert it into a state-dependent reward $r(s)$:
  $$
  r(s, a) = \mathbb{E}_{a \sim \pi(a \mid s)} \left[ \hat{r}(s, a) \right].
  $$
</details> -->

The goal of RL is to learn a policy $$\pi: \mathcal{S} \to \Delta(\mathcal{A})$$ that maximizes the expected discounted return 

$$
\begin{align}
J(\pi) = (1 - \gamma) \mathbb{E}_{\tau \sim \pi(\tau)} \left[ \sum_{t = 0}^{\infty} \gamma^t r(s_t) \right],
\label{eq:rl-obj}
\end{align}
$$

where $\tau$ is a trajectory sampled by the policy $$\pi$$. Alternatively, we can swap the discounted sum over rewards into a discounted sum over states and use it to describe the expected discounted return. Namely, the discounted sum over states is called *the discounted state occupancy measure*<d-cite key="touati2021learning,janner2020gamma,eysenbach2022contrastive,zheng2023contrastive"></d-cite>, i.e., the *successor measures*<d-cite key="dayan1993improving,barreto2017successor"></d-cite>,
<!-- We can interpret the expected discounted return as first sampling trajectories from the distribution over state squences and then summing *the discounted rewards* over different sequences. Since the reward function is time-invariant, we can instead swap the summation over rewards into a summation over the distribution of state sequences, reinterpreting the expected discounted return as first sampling future state from *the discounted distribution over states* and then evaluate the reward at that state. Prior work defined the discounted distribution over states as the discounted state occupancy measure<d-cite key="touati2021learning,janner2020gamma,eysenbach2022contrastive,zheng2023contrastive"></d-cite>, i.e., the successor measures<d-cite key="dayan1993improving,barreto2017successor"></d-cite>, -->
$$
\begin{align}
p_{\gamma}^{\pi}(s) = (1 - \gamma) \sum_{t = 0}^{\infty} \gamma^t p^{\pi}_t(s),
\label{eq:occ-measure}
\end{align}
$$

where $p^{\pi}_t(s)$ is the probability of visiting state $s$ at exact time step $t$ following policy $\pi$, and note that the probability of visiting state $s$ at the beginning is $p^{\pi}_0(s) \triangleq p_0(s) $.<d-footnote>The summation in Eq.$~\ref{eq:occ-measure}$ starts from the current time step<d-cite key="eysenbach2022contrastive,touati2021learning"></d-cite> instead of the next time step as in some prior approaches<d-cite key="janner2020gamma,zheng2024contrastive"></d-cite>.</d-footnote> Using the discounted state occupancy measure, we can rewrite the expected discounted return as 

$$
\begin{align*}
J(\pi) = \mathbb{E}_{s \sim p_{\gamma}^{\pi}(s)}\left[ r(s) \right].
\end{align*}
$$

This alternative definition implies that maximizing the expected discounted return is equivalent to maximizing the expected reward under the discounted occupancy measure.


A goal-conditioned MDP (GC-MDP) is a special case of an MDP where the reward function takes the form $$r(s,g) = \mathbb{1}[s=g]$$, indicating whether the agent has reached a specified goal state $$g$$. For such a GC-MDP, the expected discounted return can be expressed as:

$$
J(\pi) = \mathbb{E}_{\pi, \rho_0}\left[\sum_{t=0}^\infty \gamma^{\,t} r(s_t,g) \mid s_0\right] = d^\pi_\gamma(g),
$$

which shows that maximizing the return is equivalent to maximizing the goal-reaching probability under the discounted occupancy measure. Our key insight is to construct a GC-MDP where the task of reaching a synthetic absorbing state $$s^+$$ becomes equivalent to maximizing the expected return in the original MDP. This reformulation allows us to leverage goal-conditioned reinforcement learning algorithms to solve standard return-maximization problems.

# MDP to GCMDP

## Constructing the Goal-Conditioned MDP

We construct a new GC-MDP $$\widetilde{\mathcal{M}} = (\widetilde{\mathcal{S}}, \mathcal{A}, \tilde{p}, \tilde{r}, \rho_0, \tilde{\gamma})$$ such that maximizing the probability of reaching a single absorbing state $$s^+$$ corresponds exactly to maximizing expected return in the original MDP. The state space augments the original MDP with two additional absorbing states:

$$
\widetilde{\mathcal{S}} = \mathcal{S} \cup \{s^+, s^-\},
$$

where $$s^+$$ represents a successful terminal state and $$s^-$$ represents a failure terminal state. Both states are absorbing, meaning that once the agent reaches either state, it remains there indefinitely.

For all $$s \in \mathcal{S}$$:

$$
\tilde{p}(s' \mid s,a)
=
\begin{cases}
s^+, &\text{with probability } (1-\alpha)\, r(s) \\
s^-, &\text{with probability } (1-\alpha)\, (1-r(s)) \\
p(s'|s,a), &\text{with probability } \alpha
\end{cases}
$$

Where $$\alpha \in (0, 1)$$
Absorbing states satisfy:

$$
\tilde{p}(s'|s^+,a)=\mathbf{1}(s'=s^+), \\
\tilde{p}(s'|s^-,a)=\mathbf{1}(s'=s^-).
$$



At each time step, with probability $$\alpha$$ the agent follows the original dynamics, and with probability $$1-\alpha$$ the episode terminates by jumping to either $$s^+$$ or $$s^-$$.




## Relating The goal reaching probability to the reward maximization Return

Let $$t_{s^+}$$ denote the first time the agent enters $$s^+$$. For $$k\ge 1$$:

$$
\begin{align*}
\tilde{p}^{\pi}(t_{s^+} = k) &= \sum_{s' \in \mathcal{S}} \tilde{p}^{\pi}(s_{k-1}=s') \,\tilde{p}(s^+|s') \\
&=  \sum_{s' \in \mathcal{S}} \tilde{p}^{\pi}(s_{k-1}=s')\,(1-\alpha) \,r(s') \\
&= \sum_{s' \in \mathcal{S}} \alpha^{k-1} p^{\pi}(s_{k-1}=s')\,(1-\alpha) \,r(s')
\end{align*}
$$

The last equality follows because for the agent to be in a non-absorbing state $$s'$$ at time $$k-1$$ in the GC-MDP, it must have avoided transitioning to any absorbing state in all previous timesteps, which occurs with probability $$\alpha^{k-1}$$, while following the dynamics of the original MDP to reach $$s'$$.


The $$\tilde{\gamma}$$-discounted occupancy of $$s^+$$ is:


$$
\begin{align*}
\tilde{d}^\pi_{\tilde{\gamma}}(s^+ \mid s_0) &=(1-\tilde{\gamma})\sum_{t\geq 0} \tilde{\gamma}^t\tilde{p}^\pi(s_t=s^+ \mid s_0)\\
&=(1-\tilde{\gamma})\sum_{t\geq 0} \tilde{\gamma}^t\tilde{p}^\pi(t_{s^+}\leq t \mid s_0)\\
&=(1-\tilde{\gamma})\sum_{t\geq 0} \tilde{\gamma}^t\sum_{k \leq t}\tilde{p}^\pi(t_{s^+}= k \mid s_0)\\
&= (1-\tilde{\gamma}) \sum_{k \geq 1} \tilde{p}^{\pi}(t_{s^+} = k\mid s_0) \sum_{t\geq k}\tilde{\gamma}^t \\
&=\sum_{k \geq 1} \tilde{\gamma}^k \tilde{p}^{\pi}(t_{s^+} = k\mid s_0) \\
&=  (1-\alpha) \,\tilde{\gamma}\, \sum_{k \geq 1} \tilde{\gamma}^{k-1}  \sum_{s' \in \mathcal{S}} \alpha^{k-1} p^{\pi}(s_{k-1}=s'\mid s_0)\,r(s')\\
&= (1-\alpha) \,\tilde{\gamma}\, \sum_{s' \in \mathcal{S}} r(s') \sum_{k \geq 0} (\tilde{\gamma}\alpha)^{k} \,p^{\pi}(s_{k}=s'\mid s_0)\\
&= \frac{(1-\alpha) \,\tilde{\gamma} }{1-\alpha \tilde{\gamma}}\,\mathbb{E}_{d^\pi_\gamma(s\mid s_0)}[r(s)]
\end{align*}
$$

Recognizing that the term $$\sum_{k\geq 0} (\alpha\tilde{\gamma})^k p^\pi(s_k=s')$$ represents the occupancy measure of the original MDP with an effective discount factor of $$\gamma = \alpha \tilde{\gamma}$$, we obtain the final result. Thus, maximizing the original return with discount factor $$\gamma$$ is equivalent to maximizing $$\tilde{d}^\pi_{\tilde{\gamma}}(s^+)$$ in the constructed GC-MDP, where:


$$
\gamma = \alpha\,\tilde{\gamma}.
$$

<!-- Moreover, note that our results implies that if the disocunt factor of teh goal-reaching MDP i.e.,, $$\tilde{\gamma}=1$$ where equivalently we are in a relaim where if we use a reward ffunction $$-\mathbf{1}(s'\neq s^+)$$ the Q value of teh goal reaching MDP is exactly the negative hitting time of the goal and in this setup the  -->

Hence, we have established the equivalence:

$$
J(\pi) = d^\pi_{\tilde{\gamma}}(s^+)
$$

where $$J(\pi)$$ is the reward maximization objective in the original MDP.



<div style="background-color: #E3F2FD; padding: 15px; border-left: 4px solid #1E88E5; margin: 20px 0;">
<span style="color: #1565C0; font-weight: 600;">üìù Note on The Conversion:</span>
<ul style="color: #424242; margin-top: 8px;">
<li><strong>Non-normalized rewards:</strong> For rewards $$r(s) \in [r_{\min}, r_{\max}]$$, apply normalization: $$\hat{r}(s) = \frac{r(s) - r_{\min}}{r_{\max} - r_{\min}}$$ to transform to $$[0,1]$$.</li>
<li><strong>State-action rewards:</strong> For $$r(s,a)$$, marginalize over actions: $$\hat{r}(s) = \mathbb{E}_{a \sim \pi(\cdot|s)}[r(s,a)]$$ to convert to state-only rewards.</li>
</ul>
</div>



## Pratical Algorithm
In this work, we consider settings where both the reward function and the transition dynamics of the environment are known. Under this assumption, we can explicitly construct the GC-MDP using the true model: at each transition we use the reward to determine whether the agent moves to $$s^+$$, $$s^-$$, or continues according to the original dynamics. The agent then interacts directly with this constructed GC-MDP, and all data are collected from this simulated goal-conditioned environment.

However, applying this approach to large or unknown MDPs is more challenging. When the reward function is not available a priori, we cannot directly construct the GC-MDP. One option is to learn a model of the environment and use it to simulate the GC-MDP, but this requires a full model-based RL pipeline, which is often undesirable in complex domains.

A more scalable alternative is **data augmentation**. Instead of learning a model and simulating the GC-MDP, the agent interacts with the true environment while we transform the collected trajectories so they mimic GC-MDP rollouts. Specifically:

- For each trajectory collected from the original MDP, sample a geometric random variable with parameter $$1-\gamma$$ to determine the timestep at which the GC-MDP would terminate.
- At that timestep, look at the observed reward and flip a coin with probability $$r$$ of transitioning to $$s^+$$ (and $$1-r$$ to $$s^-$$).
- Truncate the trajectory at that sampled timestep and append the absorbing transition.

Since all non-absorbing transitions in the GC-MDP match those in the original MDP, this augmentation procedure produces trajectories consistent with GC-MDP interaction without requiring a learned transition model.

We leave the implementation and evaluation of this augmentation strategy to future work.

# Experiments

We conduct experiments in the Riverswim environment, which requires exploration to reach the end of the river (linear chain of states) by choosing to move right at each state. The agent receives a reward of 1.0 in the rightmost state and receives a small distractor reward of 0.005 if it moves left at any other state. To avoid policy initialization bias, we randomize which action moves left vs. right at each state. 

We train SGCRL on an goal-conditioned version of Riverswim augmented with a positive and negative absorbing state with $$\gamma = 0.95$$. The original MDP and augmented GCMDP transition dynamics are shown in Figure 2.

{% include figure.liquid path="assets/img/2026-04-27-GCMDP/mdp.png" class="img-fluid" %}
{% include figure.liquid path="assets/img/2026-04-27-GCMDP/gcmdp.png" class="img-fluid" %}
**Figure 1.** Transition dynamics for original Riverswim MDP with 6 states (top) and augmented GCMDP (bottom). The transition dynamics of the GCMDP are determined by the original dynamics, the discount factor $$\gamma$$, and the reward function.

We compare SGCRL to a baseline Deep Q-learning (DQN) and a baseline Proximal Policy Optimization (PPO) agent trained in the standard Riverswim environment (with explicit rewards). For all training performance curves, we plot the success rate of the policy in the augmented environment - the rate at which the agent reaches the positive absorbing goal state. We also plot the mean augmented success rate for an oracle agent that always takes the optimal policy. We find that for shorter river lengths and shorter horizons, DQN converges faster and achieves higher returns than SGCRL. However, as river length and horizon increase, SGCRL achieves similar or higher returns than DQN (Fig. 2). This results suggest that, while SGCRL is not sample efficient for short-horizon exploration tasks, the GC-MDP formulation enables improved exploration in longer-horizon exploration tasks. 


<!-- ![riverswim_results](./riverswim_results.png) -->
{% include figure.liquid path="assets/img/2026-04-27-GCMDP/riverswim_results.png" class="img-fluid" %}

**Figure 2.** Success rate of DQN, PPO, SGCRL, and oracle in the augmented GC-MDP. Success rate is averaged over 5 seeds with standard error shown.

---

