<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="introduction">Introduction</h2> <p>Vision-language models (VLMs) have witnessed remarkable progress in recent years, with state-of-the-art systems demonstrating impressive capabilities across a wide range of tasks，such as content recognition, video understanding, multimodal reasoning， while becoming increasingly efficient and compact <d-cite key="vteam2025glm45vglm41vthinkingversatilemultimodal"></d-cite> <d-cite key="kimiteam2025kimivltechnicalreport"></d-cite>. This advancement has spurred interest in adapting VLMs to process complex and lengthy textual contexts—a domain traditionally handled by text-only large language models (LLMs). The approach of representing text as a visual input is compelling for two primary reasons. <strong><em>Conceptually</em></strong>, it mimics the human cognitive process of reading by recognizing the visual patterns of words and sentences. <strong><em>Technically</em></strong>, it enables input compression through methods like reducing resolution and font size when rendering texts into images, which can preserve the essence of the text. In this blogpost, we first review existing literature on converting textual information into visual formats. We then evaluate VLMs on more challenging long-context understanding benchmarks than those used in prior studies to assess their capabilities and limitations when processing extensive textual information. Our results indicate that (describe the key finding here). Finally, we explore whether performance can be improved by focusing on important contexts through methods like text retrieval and locating salient image patches, leading to deeper understanding of the crucial factors that influence VLM performance on this particular task. We hope this work motivates future research toward developing more capable models for visual long-context understanding.</p> <h2 id="background">Background</h2> <h3 id="visual-language-models">Visual Language Models</h3> <p>Visual language models (VLMs) are AI systems that jointly process images and text to output text, widely applied in tasks like image captioning, visual question answering, and multimodal reasoning. State-of-the-art VLMs are typically built by combining a Large Language Model (LLM) with a vision encoder that process visual inputs (e.g. images, videos). Such VLMs consist of a vision encoder, a language model, and a cross-modal alignment module. Specifically, the vision encoder (often a Vision Transformer <d-cite key="dosovitskiy2021imageworth16x16words"></d-cite>) is first applied to extract visual features from input images. These visual features are then mapped into the language model’s embedding space via a projection network (e.g. a small MLP <d-cite key="kimiteam2025kimivltechnicalreport"></d-cite>), where finally the language model (usually adopting a decoder-only architecture <d-cite key="brown2020languagemodelsfewshotlearners"></d-cite>) integrates both the text and projected visual features to produce the final text output. Figure 1 shows the architecture of Qwen 2.5 VL <d-cite key="bai2025qwen25vltechnicalreport"></d-cite>, a family of capable VLMs developed built with this design, utilizing a redesigned vision transformer architecture and the Qwen 2.5 LLM backbone <d-cite key="qwen2025qwen25technicalreport"></d-cite> to achieve competitive performance on a wide range of multimodal tasks.</p> <div class="row justify-content-center"> <div style="height: 70%"> <figure> <picture> <source class="responsive-img-srcset" srcset="/GCMDP-blogpost/assets/img/2026-04-27-visual-long-context/qwen25_vl-480.webp 480w,/GCMDP-blogpost/assets/img/2026-04-27-visual-long-context/qwen25_vl-800.webp 800w,/GCMDP-blogpost/assets/img/2026-04-27-visual-long-context/qwen25_vl-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/GCMDP-blogpost/assets/img/2026-04-27-visual-long-context/qwen25_vl.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="l-gutter caption" style="width: 150%"> <b>Figure 1:</b> Architecture of Qwen 2.5 VL. </div> <h3 id="long-context-language-models">Long Context Language Models</h3> <p>Long Context Language Models refer to LLMs geared towards processing very large inputs, such as lengthy documents or extended conversations that far exceed the few-thousand-token contexts of standard models. Long-context tasks require LLMs have strong memory and focus – the ability to identify and retain key details scattered across massive input contexts, and to integrate those details when formulating an answer or summary. They also require handling of topics that evolve over the context, meaning the model should track entities and narratives over long ranges and not “forget” crucial pieces midway. This capability is increasingly important: many real-world use cases involve lengthy inputs (legal contracts, large codebases, customer service logs, etc.), where an LLM that can effectively utilize very long contexts and produce little hallucination is crucial for downstream deployment.</p> <p>Achieving reliable long-context understanding is challenging for several reasons. First, the computational cost of the transformer’s self-attention grows quadratically with input, making naive processing of massive input length extremely expensive. While recent advances in training foundation models extended the context window to 128k even 1M tokens <d-cite key="comanici2025gemini25pushingfrontier"></d-cite>, performance degradation on challenging long context benchmarks is still observed when the input length is too long, even for the best available LLMs <d-cite key="li2025getscitedmostbenchmarking"></d-cite> <d-cite key="modarressi2025nolimalongcontextevaluationliteral"></d-cite>. Furthermore, long context modeling becomes even more complex in the multimodal setting, where a VLM needs to analyze lengthy documents containing many pages of text and images, or a long video with hundreds of frames <d-cite key="fu2025ocrbenchv2improvedbenchmark"></d-cite> <d-cite key="wu2024longvideobenchbenchmarklongcontextinterleaved"></d-cite>.</p> <p>To quantitatively assess long context capabilities in LLMs, the “Needle-in-a-Haystack” (NIAH) test has been proposed for evaluating long context capabilities of LLMs <d-cite key="kamradt2023needle"></d-cite>. This test involves inserting a specific, often arbitrary fact (needle) at various depths within a massive corpus of unrelated text (haystack) and subsequently querying the model to retrieve it to verify recall fidelity. The test is widely adopted for evaluation in existing works due to the ability to construct questions with controllable and unbounded input context lengths, with intuitive visualizations that allow immediate inspection of model’s performance at different context lengths and needle placement depths. Despite its utility as an effective evaluation benchmark, the scope NIAH test is limited to stress-testing model’s ability on synthetic tasks when given very long context lengths. The context of NIAH tests is synthesized with most texts being noisy, an assumption that does not hold for real-world long context applications. Furthermore, despite more sophiscated NIAH tests being proposed to test advanced reasoning capabilities <d-cite key="modarressi2025nolimalongcontextevaluationliteral"></d-cite>, these tests still rely on pinpointing multiple needles in a single haystack. A model that scores well on NIAH tests might not perform well on tasks requiring understanding of the entire context such as multi-document summarization.</p> <h2 id="why-process-text-as-images">Why Process Text as Images?</h2> <p>Intuitively, it is odd to go through the dirty work of rendering text as images and then process them with VLMs, when we can directly use capable LLMs to process such texts. Nevertheless, this seeming contradiction is resolved when recognizing the fundamental architectural bottlenecks and representational limitations inherent to fixed-vocabulary, token-based models. While the idea itself has not been explored in depth nor shown any promising results, it carries significant research value that tackles important scaling challenges in modern LLMs, offering bevnefits from both a practical efficiency standpoint and a foundational theoretical perspective. In this section, we will first discuss the empirically proven benefits of visual text representation, then migrate to discussing the more fundamental, unrealized potentials of such an approach when compared to standard text processing.</p> <p><strong><em>Practical Perspective: Efficiency and Universality</em></strong> One of the primary benefits of visual text representation is compressing input tokens, which is crucial for allowing LLMs to process more information under limited context lengths. Such conversion can be done with simple software engineering efforts and no need for specialized hardware, where previous works generally build pipelines around relevant softwares like Latex TypeSetter and PDF generator to obtain images of texts <d-cite key="cheng2025glyphscalingcontextwindows"></d-cite> <d-cite key="li2025textpixelstakeshalf"></d-cite>. Depending on hyperparameter settings, conversion can achieve different token reduction ratios ranging from 1.2x to 7.7x for English texts <d-cite key="xing2025texttokenizationvisualreading"></d-cite>. Figure 2 shows an example of how 1178 tokens worth of text can be compressed to about 250 tokens by rendering it into one image. This compression directly enable tangible inference speed gains, substantially reducing FLOPs and latency across question-answering tasks. Beyond English, this visual representation also theorectically achieves infinite vocabulary, bypassing the limitations of fixed vocabulary sizes of existing LLMs. Experiments have shown that in multilingual translation tasks, using visual representation achieved similar performances while treating different languages fairly, without excessively fragmenting low-resource language texts <d-cite key="xing2025texttokenizationvisualreading"></d-cite>.</p> <div class="row justify-content-center"> <div style="height: 70%"> <figure> <picture> <source class="responsive-img-srcset" srcset="/GCMDP-blogpost/assets/img/2026-04-27-visual-long-context/compression-480.webp 480w,/GCMDP-blogpost/assets/img/2026-04-27-visual-long-context/compression-800.webp 800w,/GCMDP-blogpost/assets/img/2026-04-27-visual-long-context/compression-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/GCMDP-blogpost/assets/img/2026-04-27-visual-long-context/compression.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="l-gutter caption" style="width: 150%"> <b>Figure 2:</b> An example of how text-to-image compression saves input tokens. Tokens are measured by the gemma 3 tokenizer. </div> <p><strong><em>The Potential for Foundational Study: Expressiveness and Unification</em></strong> Visual text representation has more potential beyond established gains and benefits confirmed by existing studies. One such potential lies in the information density encoded in the images compared to text tokens. Visual encoding incorporates meta-linguistic features like highlighting, font colors, annotations, etc., which can potentially used as important cues when performing sophisticated reasoning while introducing no additional tokens. Such structural cues cannot be used for text tokens without introducing additional tokens for describing the structure. Furthermore, since rendering texts to images can be considered as lossless in information if the final image contains all original texts in the same structure, it allows unifying modalities and subsequently building vision-centric models that better capture cross-modal relationships through a single, unified representation space.</p> <h2 id="limitation-and-towards-better-understanding">Limitation and Towards Better Understanding</h2> <p>Despite the promising picture we have outlined for visual text representation where rendering text as images enables massive context scaling and efficiency gains, such systems remain in an exploratory stage. To bridge the gap between “promising prototype” and “production-ready infrastructure,” we must rigorously evaluate their limitations. In this blogpost, we explore in depth one major bottleneck of existing work: insufficient evaluation on long context capabilities.</p> <p>While different existing works have different focuses, we observe works that evaluate long context capabilities either do not use long enough inputs or rely heavily on “Needle-in-a-Haystack” (NIAH) tests such as RULER <d-cite key="hsieh2024rulerwhatsrealcontext"></d-cite>. For example, <d-cite key="cheng2025glyphscalingcontextwindows"></d-cite> conducts experiments on LongBench <d-cite key="bai2024longbenchbilingualmultitaskbenchmark"></d-cite> where context lengths are about 10k-20k tokens, while <d-cite key="xing2025visioncentrictokencompressionlarge"></d-cite> conducts experiments on tasks with inputs up to approximately 16k tokens. Context lengths at this level are not enough to convincingly test the long context capabilities with visual text representation when many modern VLMs can already process 32k-128k tokens <d-cite key="vteam2025glm45vglm41vthinkingversatilemultimodal"></d-cite> <d-cite key="kimiteam2025kimivltechnicalreport"></d-cite>. Regarding the issue of NIAH tests, different evaluations vary greatly in difficulty, making it difficult to understand the true capabilities of VLMs on long context tasks. For example, <d-cite key="li2025textpixelstakeshalf"></d-cite> evaluated VLMs on the RULER S-NIAH (single retrieval) task with no more than 4k tokens and observed near perfect performance, but the single retrieval setting and the short context length cannot guarantee similar performance when evaluated on harder tasks with longer context lengths. <d-cite key="cheng2025glyphscalingcontextwindows"></d-cite> conducted experiments on the MRCR NIAH test with 128k-1M context lengths and observed low accuracies for both visual text representation and standard text baselines, but the limited range of models and lack of ablation studies provide little clues on how these models fail.</p> <p>While existing studies have different focuses, we notice that works testing long-context capabilities either use insufficiently long inputs or overly rely on NIAH tests like RULER <d-cite key="hsieh2024rulerwhatsrealcontext"></d-cite>. For example, <d-cite key="cheng2025glyphscalingcontextwindows"></d-cite> experiments on LongBench <d-cite key="bai2024longbenchbilingualmultitaskbenchmark"></d-cite> use context lengths of only 10k-20k tokens. Similarly, <d-cite key="xing2025visioncentrictokencompressionlarge"></d-cite> conducts experiments on tasks with inputs up to approximately 16k tokens. Such context lengths are inadequate for convincingly testing the long-context capabilities of visual text representation, especially since many modern VLMs (Vision-Language Models) can already process 32k-128k tokens <d-cite key="vteam2025glm45vglm41vthinkingversatilemultimodal"></d-cite> <d-cite key="kimiteam2025kimivltechnicalreport"></d-cite>. Regarding the issue with NIAH tests, the varying difficulty of different evaluations makes it hard to understand the true capabilities of VLMs on long-context tasks. For instance, <d-cite key="li2025textpixelstakeshalf"></d-cite> evaluated VLMs on the RULER S-NIAH (single retrieval) task with no more than 4k tokens and observed near-perfect performance. However, this single-retrieval setting and the short context length cannot guarantee similar performance on harder tasks with longer contexts. Furthermore, <d-cite key="cheng2025glyphscalingcontextwindows"></d-cite> conducted experiments on the MRCR NIAH test using 128k-1M context lengths and observed low accuracies for both visual text representation and standard text baselines. Nevertheless, the limited model selection and lack of ablation studies offer little insight into why these models fail on these tasks.</p> <p>To address the issue on insufficent long context evaluation, we move beyond Needle-in-a-Haystack (NIAH) tests and employ the Loong benchmark <d-cite key="wang2024loong"></d-cite> for comprehensive ablation studies. Unlike NIAH, Loong inputs consist of multiple real documents with context lengths up to 250k tokens, with many questions frequently exceeding 100k tokens. Questions vary in difficulty from single-fact retrieval to clustering and multi-document reasoning, providing a more nuanced evaluation than a single accuracy metric. We also expand the experimental scope beyond fixed hyperparameters, testing various architectures, model sizes, reasoning lengths, image rendering settings, retrieval methods, and text-only baselines. These results aim to clarify effective strategies for visual text representation in long-context tasks, guiding the development of more practical systems.</p> <h2 id="experimental-setup-and-result-analysis">Experimental Setup and Result Analysis</h2> <p>In this section, we will first describe the experimental setup in detail, then present the results and discuss the findings.</p> <h3 id="experimental-setup">Experimental Setup</h3> <ul> <li> <strong>Dataset</strong>: We use the Loong benchmark <d-cite key="wang2024loong"></d-cite> for evaluation. As we exclusively focus on long context capabilities, we only keep questions with context length over 100k tokens in Loong. In total, we have 713 questions with 481 questions having context length 100-200k tokens and 232 questions having context length over 200k tokens. Loong consists of 4 types of tasks: <ul> <li> <em>Spotlight Locating</em>: Finding evidence in one relevant document among many for question answering.</li> <li> <em>Comparison</em>: Locating evidences across multiple documents and subsequently correlating and comparing them.</li> <li> <em>Clustering</em>: Aggregating evidences across multiple documents based on specific criteria.</li> <li> <em>Chain of Reasoning</em>: Finding evidences across multiple documents and resolving the logical connections between them.</li> </ul> </li> <li> <strong>Model</strong>: We evaluate the performance of recent capable VLMs including the Qwen 3 VL model family<d-footnote>https://github.com/QwenLM/Qwen3-VL</d-footnote> and the Gemma 3 model family <d-cite key="gemmateam2025gemma3technicalreport"></d-cite>. Additionally, we also evaluate the performance of the Glyph model fine-tuned specifically for visual text representation <d-cite key="cheng2025glyphscalingcontextwindows"></d-cite>. Qwen 3 VL models have 256k context length, while Gemma 3 models and Glyph have 128k context length.</li> <li> <strong>Rendering Settings</strong>: We employ the optimal rendering settings found in <d-cite key="cheng2025glyphscalingcontextwindows"></d-cite> for all models.</li> <li> <strong>Inference Settings</strong>: At test time, we truncate the context to fit the context window of the tested model. For inference with images, we keep the first n images that can fit the context window. For pure text inference, we keep the first n tokens that can fit the context window.</li> <li> <strong>Evaluation</strong>: We follow the same evaluation protocol as originally described in Loong. We use the Gemini 2.5 Pro model instead of GPT-4 originally used in Loong for better instruction following. For each question, the judge model will output a score between 1-100 that indicates the quality of the answer. We also calculate the percentage of questions that the answer is exactly the same as the ground truth and report as perfect rate.</li> </ul> <h3 id="results-and-discussion">Results and Discussion</h3> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: left">Spotlight Locating</th> <th style="text-align: left">Comparison</th> <th style="text-align: left">Clustering</th> <th style="text-align: left">Chain of Reasoning</th> <th style="text-align: left">Overall</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Qwen3-VL-32B-Instruct</td> <td style="text-align: left">34.96 / 28.7%</td> <td style="text-align: left">20.12 / 12.6%</td> <td style="text-align: left">9.35 / 1.5%</td> <td style="text-align: left">12.64 / 5.4%</td> <td style="text-align: left">16.50 / 9.1%</td> </tr> <tr> <td style="text-align: left">Qwen3-VL-30B-A3B-Instruct</td> <td style="text-align: left">24.65 / 20.0%</td> <td style="text-align: left">16.63 / 9.5%</td> <td style="text-align: left">5.62 / 0.5%</td> <td style="text-align: left">6.05 / 1.8%</td> <td style="text-align: left">11.06 / 5.8%</td> </tr> <tr> <td style="text-align: left">Qwen3-VL-8B-Instruct</td> <td style="text-align: left">25.60 / 17.5%</td> <td style="text-align: left">11.14 / 4.2%</td> <td style="text-align: left">5.90 / 0.5%</td> <td style="text-align: left">8.58 / 2.7%</td> <td style="text-align: left">10.83 / 4.6%</td> </tr> <tr> <td style="text-align: left">Gemma-3-27B-Instruct</td> <td style="text-align: left">35.75 / 31.2%</td> <td style="text-align: left">15.02 / 10.5%</td> <td style="text-align: left">4.61 / 0.5%</td> <td style="text-align: left">7.10 / 3.6%</td> <td style="text-align: left">12.43 / 8.3%</td> </tr> <tr> <td style="text-align: left">Gemma-3-12B-Instruct</td> <td style="text-align: left">26.49 / 22.5%</td> <td style="text-align: left">6.66 / 2.1%</td> <td style="text-align: left">3.18 / 0.0%</td> <td style="text-align: left">7.02 / 1.8%</td> <td style="text-align: left">8.64 / 4.6%</td> </tr> <tr> <td style="text-align: left">Glyph-9B</td> <td style="text-align: left">41.29 / 37.5%</td> <td style="text-align: left">20.91 / 14.7%</td> <td style="text-align: left">6.45 / 0.0%</td> <td style="text-align: left">15.19 / 4.5%</td> <td style="text-align: left">17.14 / 10.2%</td> </tr> </tbody> </table> <div class="l-gutter caption" style="width: 150%"> <b>Table 1:</b> Results of different VLMs on the Loong benchmark for context length 100-200k tokens, tested with default settings. Performance is reported as "Judge Model Score / Perfect Rate". Tasks are ranked by difficulty from left to right, where Spotlight Locating is the easiest task and Chain of Reasoning is the hardest task. </div> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: left">Spotlight Locating</th> <th style="text-align: left">Comparison</th> <th style="text-align: left">Clustering</th> <th style="text-align: left">Chain of Reasoning</th> <th style="text-align: left">Overall</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Qwen3-VL-32B-Instruct</td> <td style="text-align: left">47.89 / 34.0%</td> <td style="text-align: left">28.12 / 17.5%</td> <td style="text-align: left">7.60 / 0.0%</td> <td style="text-align: left">4.21 / 1.8%</td> <td style="text-align: left">18.47 / 10.3%</td> </tr> <tr> <td style="text-align: left">Qwen3-VL-30B-A3B-Instruct</td> <td style="text-align: left">33.45 / 23.4%</td> <td style="text-align: left">8.57 / 2.5%</td> <td style="text-align: left">5.57 / 1.1%</td> <td style="text-align: left">10.89 / 3.5%</td> <td style="text-align: left">13.04 / 6.5%</td> </tr> <tr> <td style="text-align: left">Qwen3-VL-8B-Instruct</td> <td style="text-align: left">27.81 / 23.4%</td> <td style="text-align: left">16.52 / 7.5%</td> <td style="text-align: left">4.75 / 0.0%</td> <td style="text-align: left">5.23 / 3.5%</td> <td style="text-align: left">11.57 / 6.9%</td> </tr> <tr> <td style="text-align: left">Gemma-3-27B-Instruct</td> <td style="text-align: left">28.83 / 21.3%</td> <td style="text-align: left">12.18 / 7.5%</td> <td style="text-align: left">4.68 / 0.0%</td> <td style="text-align: left">1.39 / 0.0%</td> <td style="text-align: left">10.06 / 5.6%</td> </tr> <tr> <td style="text-align: left">Gemma-3-12B-Instruct</td> <td style="text-align: left">11.28 / 6.4%</td> <td style="text-align: left">11.60 / 5.0%</td> <td style="text-align: left">3.20 / 0.0%</td> <td style="text-align: left">0.98 / 0.0%</td> <td style="text-align: left">5.74 / 2.2%</td> </tr> <tr> <td style="text-align: left">Glyph-9B</td> <td style="text-align: left">40.60 / 27.7%</td> <td style="text-align: left">13.85 / 10.0%</td> <td style="text-align: left">6.02 / 0.0%</td> <td style="text-align: left">4.19 / 0.0%</td> <td style="text-align: left">13.93 / 7.3%</td> </tr> </tbody> </table> <div class="l-gutter caption" style="width: 150%"> <b>Table 2:</b> Results of different VLMs on the Loong benchmark for context length above 200k tokens, tested with default settings. Performance is reported as "Judge Model Score / Perfect Rate". Tasks are ranked by difficulty from left to right, where Spotlight Locating is the easiest task and Chain of Reasoning is the hardest task. </div> <p>Tables 1 and 2 present the results of various VLMs on the Loong benchmark for context lengths of 100–200k tokens and those exceeding 200k tokens, respectively. Within the Qwen and Gemma families, larger models generally outperform smaller ones across all tasks. Notably, the Glyph model achieves the best overall performance despite having only 9B parameters and a 128k context limit, demonstrating the effectiveness of dedicated fine-tuning for visual text representation. Nevertheless, we use Instruct versions of Qwen and Gemma models for evaluation, while Glyph is fine-tuned from the GLM-4.1V-9B reasoning model. Consequently, Glyph’s performance advantage may also stem from enhanced reasoning capabilities through generating more tokens at test time.</p> <p>Regarding model architecture, we observe that within the Qwen 3 family, Mixture-of-Experts (MoE) models produce inferior results compared to dense models of similar size. Specifically, the 30B MoE model underperforms the 32B dense model and is only marginally superior to the 8B dense version. In terms of context capacity, VLMs limited to 128k tokens exhibit significant performance degradation on inputs exceeding 200k tokens. In contrast, Qwen 3 VL models with a 256k context window perform slightly better on inputs over 200k tokens than on the 100–200k range. This suggests that, despite the compression offered by visual encoding, an extended context window remains crucial for processing ultra-long contexts.</p> <div class="row justify-content-center"> <div style="height: 70%"> <figure> <picture> <source class="responsive-img-srcset" srcset="/GCMDP-blogpost/assets/img/2026-04-27-visual-long-context/dpi_reasoning_ablation-480.webp 480w,/GCMDP-blogpost/assets/img/2026-04-27-visual-long-context/dpi_reasoning_ablation-800.webp 800w,/GCMDP-blogpost/assets/img/2026-04-27-visual-long-context/dpi_reasoning_ablation-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/GCMDP-blogpost/assets/img/2026-04-27-visual-long-context/dpi_reasoning_ablation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="l-gutter caption" style="width: 150%"> <b>Figure 3 (Left):</b> Overall scores of different VLMs on the Loong benchmark for context length 100-200k tokens, tested with different DPI settings. Performance is reported as "Judge Model Score / Perfect Rate". Tasks are organized by difficulty, where Spotlight Locating is the easiest task and Chain of Reasoning is the hardest task. <br> <b>Figure 4 (Right):</b> Overall scores of different Instruct and Thinking VLMs on the Loong benchmark. </div> <p>To investigate how compression rates affect performance, we conduct an ablation study using higher DPI rendering and text-only inputs, with results shown in Figure 3. We observe that for both the Qwen 3 and Gemma 3 families, the text-only baseline significantly outperforms image-based inputs. This indicates that these VLMs remain more effective at processing raw text despite it offering no compression. When changing rendering DPI, Gemma 3 shows no change in performance which aligns with its strategy of resizing images to fixed dimensions and token counts. In contrast, Qwen 3 and Glyph that process images at native resolutions exhibit distinct behaviors: Qwen 3 benefits from higher DPI, whereas Glyph’s performance declines. We hypothesize this discrepancy stems from context window limitations. Glyph (128k context) likely suffers from information loss due to the increased token count associated with higher DPI rendering. Conversely, Qwen 3 (256k context) accommodates the higher resolution without truncation, implying that the Loong task requires retaining the full input context for optimal performance.</p> <p>To examine the impact of inference-time reasoning on long-context performance, we compared Instruct and Thinking model variants (Figure 4). We first observe that Thinking models consistently outperform their Instruct counterparts by a significant margin. This suggests that the “thinking” paradigm effectively shifts computation to the inference stage, enabling the decomposition of complex queries and deeper reasoning. Furthermore, the Qwen 3 VL 8B Thinking model surpasses Glyph, despite the latter being fine-tuned for visual text representation. Since Qwen 3 VL 8B has a longer context window and is released later than Glyph’s base model (GLM 4.1V 9B), this demonstrates that improved general reasoning and multimodal capabilities can generalize to text-as-image tasks, achieving strong performance even without specialized fine-tuning.</p> <table> <thead> <tr> <th style="text-align: left">Configuration</th> <th style="text-align: left">Overall</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Glyph</td> <td style="text-align: left">16.09 / 9.3%</td> </tr> <tr> <td style="text-align: left">Glyph + BM25 TopK 10</td> <td style="text-align: left">8.94 / 5.2%</td> </tr> <tr> <td style="text-align: left">Glyph + BM25 TopK 20</td> <td style="text-align: left">8.85 / 3.6%</td> </tr> <tr> <td style="text-align: left">Glyph + Qwen3 0.6B TopK 10</td> <td style="text-align: left">12.72 / 8.1%</td> </tr> <tr> <td style="text-align: left">Glyph + Qwen3 0.6B TopK 20</td> <td style="text-align: left">14.73 / 9.4%</td> </tr> <tr> <td style="text-align: left">Glyph + Qwen3 4B TopK 10</td> <td style="text-align: left">13.46 / 8.7%</td> </tr> <tr> <td style="text-align: left">Glyph + Qwen3 4B TopK 20</td> <td style="text-align: left">14.24 / 9.3%</td> </tr> </tbody> </table> <div class="l-gutter caption" style="width: 150%"> <b>Table 3:</b> Results for Glyph applying different retrieval strategies. </div> <p>Next, we investigate the impact of retrieval on visual text representation. Using the Glyph model as the base VLM, we evaluate BM25 (sparse) alongside Qwen 3 0.6B and 4B (dense) retrieval strategies. The input context is segmented into 1024-token chunks, and the top-$k$ passages are retrieved using the question as a query. As shown in Table 3, retrieval-based methods underperform the full-context baseline. This aligns with the original paper’s findings <d-cite key="wang2024loong"></d-cite> and our hypothesis that Loong tasks require global context understanding. Using top $k=20$ consistently outperforms $k=10$, confirming that broader context coverage is crucial. Furthermore, while dense retrieval significantly surpasses sparse methods, increasing the retriever model size yields only marginal gains.</p> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: left">Spotlight Locating</th> <th style="text-align: left">Comparison</th> <th style="text-align: left">Clustering</th> <th style="text-align: left">Chain of Reasoning</th> <th style="text-align: left">Overall</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Glyph</td> <td style="text-align: left">41.03 / 33.9%</td> <td style="text-align: left">18.81 / 13.3%</td> <td style="text-align: left">6.32 / 0.0%</td> <td style="text-align: left">11.48 / 3.0%</td> <td style="text-align: left">16.09 / 9.3%</td> </tr> <tr> <td style="text-align: left">Glyph + Qwen3 0.6B TopK 10</td> <td style="text-align: left">37.41 / 29.9%</td> <td style="text-align: left">13.01 / 10.4%</td> <td style="text-align: left">6.25 / 0.7%</td> <td style="text-align: left">4.72 / 2.4%</td> <td style="text-align: left">12.72 / 8.1%</td> </tr> <tr> <td style="text-align: left">Glyph + Qwen3 0.6B TopK 10 + Highlight</td> <td style="text-align: left">37.36 / 29.9%</td> <td style="text-align: left">20.42 / 16.3%</td> <td style="text-align: left">6.99 / 0.4%</td> <td style="text-align: left">13.79 / 4.1%</td> <td style="text-align: left">16.56 / 9.5%</td> </tr> </tbody> </table> <div class="l-gutter caption" style="width: 150%"> <b>Table 4:</b> Ablation study for highlighting retrieved passages in the entire context. </div> <p>Beyond standard retrieval, a key benefit of image-based inputs is the ability to incorporate meta-linguistic features without increasing token counts. To validate this, we implemented a mechanism that presents the full context with retrieved passages highlighted in a distinct color. We then instructed the VLMs to prioritize these highlighted sections when composing answers. As shown in Table 4, this approach yields slightly better overall performance compared to using the non-highlighted full context. Task-specific analysis reveals that while highlighting degrades performance on the simple <em>Spotlight Locating</em> task, it provides significant gains on complex tasks; notably, it outperforms the baseline by 2.31 on the challenging <em>Chain of Reasoning</em> task. This performance variance suggests that visual highlighting is most effective when the question requires a global understanding of the context rather than local retrieval. More importantly, we demonstrate that VLMs can effectively leverage meta-linguistic visual signals to handle complex, long-context tasks.</p> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, we explore the concept of representing text as images to handle long context tasks, evaluating the method against the challenging benchmark of Loong. Through comprehensive evaluation and ablation studies, we investigate how various factors influence performance, highlighting both the capabilities and limitations of the approach. In general, our findings suggest that while visual text representation is promising, a significant gap remains before it becomes suitable for real-world applications. To bridge this gap, future work should focus on developing more capable VLMs and incorporating artificial visual signals to utilize the full potential of the image modality, subsequently providing useful guidance to models in processing long contexts.</p> </body></html>