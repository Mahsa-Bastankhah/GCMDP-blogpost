<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reward maximization using Goal-conditioned RL | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="TODO - this is abstract"> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/GCMDP-blogpost/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/GCMDP-blogpost/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/GCMDP-blogpost/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/GCMDP-blogpost/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/GCMDP-blogpost/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/GCMDP-blogpost/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mahsa-bastankhah.github.io/GCMDP-blogpost/blog/2026/GCMDP/"> <script src="/GCMDP-blogpost/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/GCMDP-blogpost/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/GCMDP-blogpost/assets/js/distillpub/template.v2.js"></script> <script src="/GCMDP-blogpost/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}.detail{color:#1f77b4;cursor:pointer;display:inline-block;margin-bottom:1rem}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Reward maximization using Goal-conditioned RL",
            "description": "TODO - this is abstract",
            "published": "November 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/GCMDP-blogpost/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/GCMDP-blogpost/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/GCMDP-blogpost/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/GCMDP-blogpost/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/GCMDP-blogpost/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/GCMDP-blogpost/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/" rel="external nofollow noopener" target="_blank"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/" rel="external nofollow noopener" target="_blank">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/" rel="external nofollow noopener" target="_blank">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/" rel="external nofollow noopener" target="_blank">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Reward maximization using Goal-conditioned RL</h1> <p>TODO - this is abstract</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#problem-formulation">Problem Formulation</a> </div> <div> <a href="#pratical-algorithm">Pratical Algorithm</a> </div> <div> <a href="#experiment">Experiment</a> </div> </nav> </d-contents> <h1 id="introduction">Introduction</h1> <p>Reinforcement learning (RL) has achieved great success in solving many decision-making problems, from outperforming human-level control on challenging games<d-cite key="silver2018general"></d-cite> and fine-tuning large language models<d-cite key="shao2024deepseekmath"></d-cite> to learning scalable robotic manipulation policies<d-cite key="amin2025pi"></d-cite> Goal-conditioned reinforcement learning (GCRL) is an important type of reinforcement learning (RL) problems, which prevents hand-crafted reward functions. Instead, GCRL algorithms learn to reach different goals specified by the environment<d-cite key="kaelbling1993learning,schaul2015universal"></d-cite>. While the sparse reward structure of this task might seem to hinder exploration, recent work has demonstrated a surprising finding: contrastive RL algorithms that train with only a single challenging downstream goal‚Äîrather than a distribution of goals or using any intrinsic shaped reward‚Äîcan effectively explore the environment and acquire useful skills without any supervision <d-cite key="liu2025a,bastankhah2025"></d-cite>. This raises a natural question: can we leverage the exploration advantages of GCRL to solve standard reward maximization problems?</p> <p>In this work, we provide a positive answer by showing how any reward maximization MDP can be converted into an equivalent goal-conditioned MDP. Our construction augments the original MDP with synthetic absorbing states, where reaching the ‚Äúsuccess‚Äù absorbing state corresponds exactly to maximizing the expected return in the original problem. This equivalence enables practitioners to apply goal-conditioned RL algorithms to traditional reward maximization tasks, potentially benefiting from the superior exploration properties of GCRL methods.</p> <h1 id="problem-formulation">Problem formulation</h1> <p><strong>Markov decision process and occupancy measures.</strong> We consider a Markov decision process (MDP) defined by a state space $\mathcal{S}$, an action space $\mathcal{A}$, a transition distribution $p: \mathcal{S} \times \mathcal{A} \to \Delta(\mathcal{S})$, a reward function $r: \mathcal{S} \to [0, 1]$, an initial state distribution $p_0 \in \Delta(\mathcal{S})$, and a discount factor $\gamma \in [0, 1)$, where $\Delta(\cdot)$ denotes the set of all possible probability distributions over a space. We use \(t\) to denote the time step in MDP and assume the reward function is normalized and only depends on the state without loss of generality<d-cite key="thakoor2022generalised,ghosh2023reinforcement,frans2024unsupervised"></d-cite>.</p> <details style="background-color: #f4f4f4ff; padding: 15px; border-left: 4px solid #1E88E5; margin: 20px 0;"> <summary>Discussion about the reward function</summary> For a reward function $\hat{r}(s)$ taking values in $[r_{\min}, r_{\max}]$, we can apply the min-max normalization to transform the range into $[0, 1]$: $$ r(s) = \frac{\hat{r}(s) - r_{\min}}{r_{\max} - r_{\min}}. $$ For a reward function depending on the state-action pair $\hat{r}(s, a)$, we can marginalize over actions using the policy $\pi$ to convert it into a state-dependent reward $r(s)$: $$ r(s, a) = \mathbb{E}_{a \sim \pi(a \mid s)} \left[ \hat{r}(s, a) \right]. $$ </details> <p>The goal of RL is to learn a policy \(\pi: \mathcal{S} \to \Delta(\mathcal{A})\) that maximizes the expected discounted return</p> \[\begin{align} J(\pi) = (1 - \gamma) \mathbb{E}_{\tau \sim \pi(\tau)} \left[ \sum_{t = 0}^{\infty} \gamma^t r(s_t) \right], \label{eq:rl-obj} \end{align}\] <p>where $\tau$ is a trajectory sampled by the policy \(\pi\). Alternatively, we can swap the discounted sum over rewards into a discounted sum over states and use it to describe the expected discounted return. Namely, the discounted sum over states is called <em>the discounted state occupancy measure</em><d-cite key="touati2021learning,janner2020gamma,eysenbach2022contrastive,zheng2023contrastive"></d-cite>, i.e., the <em>successor measures</em><d-cite key="dayan1993improving,barreto2017successor"></d-cite>, \(\begin{align} p_{\gamma}^{\pi}(s) = (1 - \gamma) \sum_{t = 0}^{\infty} \gamma^t p^{\pi}_t(s), \label{eq:occ-measure} \end{align}\)</p> <p>where $p^{\pi}_t(s)$ is the probability of visiting state $s$ at exact time step $t$ following policy $\pi$, and note that the probability of visiting state $s$ at the beginning is $p^{\pi}_0(s) \triangleq p_0(s) $.<d-footnote>The summation in Eq.$~\ref{eq:occ-measure}$ starts from the current time step<d-cite key="eysenbach2022contrastive,touati2021learning"></d-cite> instead of the next time step as in some prior approaches<d-cite key="janner2020gamma,zheng2024contrastive"></d-cite>.</d-footnote> Using the discounted state occupancy measure, we can rewrite the expected discounted return as</p> \[\begin{align*} J(\pi) = \mathbb{E}_{s \sim p_{\gamma}^{\pi}(s)}\left[ r(s) \right]. \end{align*}\] <p>This alternative definition implies that maximizing the expected discounted return is equivalent to maximizing the expected reward under the discounted occupancy measure.</p> <p>A goal-conditioned MDP (GC-MDP) is a special case of an MDP where the reward function takes the form \(r(s,g) = \mathbb{1}[s=g]\), indicating whether the agent has reached a specified goal state \(g\). For such a GC-MDP, the expected discounted return can be expressed as:</p> \[J(\pi) = \mathbb{E}_{\pi, \rho_0}\left[\sum_{t=0}^\infty \gamma^{\,t} r(s_t,g) \mid s_0\right] = d^\pi_\gamma(g),\] <p>which shows that maximizing the return is equivalent to maximizing the goal-reaching probability under the discounted occupancy measure. Our key insight is to construct a GC-MDP where the task of reaching a synthetic absorbing state \(s^+\) becomes equivalent to maximizing the expected return in the original MDP. This reformulation allows us to leverage goal-conditioned reinforcement learning algorithms to solve standard return-maximization problems.</p> <h2 id="constructing-the-goal-conditioned-mdp">Constructing the Goal-Conditioned MDP</h2> <p>We construct a new GC-MDP \(\widetilde{\mathcal{M}} = (\widetilde{\mathcal{S}}, \mathcal{A}, \tilde{p}, \tilde{r}, \rho_0, \tilde{\gamma})\) such that maximizing the probability of reaching a single absorbing state \(s^+\) corresponds exactly to maximizing expected return in the original MDP. The state space augments the original MDP with two additional absorbing states:</p> \[\widetilde{\mathcal{S}} = \mathcal{S} \cup \{s^+, s^-\},\] <p>where \(s^+\) represents a successful terminal state and \(s^-\) represents a failure terminal state. Both states are absorbing, meaning that once the agent reaches either state, it remains there indefinitely.</p> <p>For all \(s \in \mathcal{S}\):</p> \[\tilde{p}(s' \mid s,a) = \begin{cases} s^+, &amp;\text{with probability } (1-\alpha)\, r(s) \\ s^-, &amp;\text{with probability } (1-\alpha)\, (1-r(s)) \\ p(s'|s,a), &amp;\text{with probability } \alpha \end{cases}\] <p>Where \(\alpha \in (0, 1)\) Absorbing states satisfy:</p> \[\tilde{p}(s'|s^+,a)=\mathbf{1}(s'=s^+), \\ \tilde{p}(s'|s^-,a)=\mathbf{1}(s'=s^-).\] <p>At each time step, with probability \(\alpha\) the agent follows the original dynamics, and with probability \(1-\alpha\) the episode terminates by jumping to either \(s^+\) or \(s^-\).</p> <h2 id="relating-the-goal-reaching-probability-to-the-reward-maximization-return">Relating The goal reaching probability to the reward maximization Return</h2> <p>Let \(t_{s^+}\) denote the first time the agent enters \(s^+\). For \(k\ge 1\):</p> \[\begin{align*} \tilde{p}^{\pi}(t_{s^+} = k) &amp;= \sum_{s' \in \mathcal{S}} \tilde{p}^{\pi}(s_{k-1}=s') \,\tilde{p}(s^+|s') \\ &amp;= \sum_{s' \in \mathcal{S}} \tilde{p}^{\pi}(s_{k-1}=s')\,(1-\alpha) \,r(s') \\ &amp;= \sum_{s' \in \mathcal{S}} \alpha^{k-1} p^{\pi}(s_{k-1}=s')\,(1-\alpha) \,r(s') \end{align*}\] <p>The last equality follows because for the agent to be in a non-absorbing state \(s'\) at time \(k-1\) in the GC-MDP, it must have avoided transitioning to any absorbing state in all previous timesteps, which occurs with probability \(\alpha^{k-1}\), while following the dynamics of the original MDP to reach \(s'\).</p> <p>The \(\tilde{\gamma}\)-discounted occupancy of \(s^+\) is:</p> \[\begin{align*} \tilde{d}^\pi_{\tilde{\gamma}}(s^+ \mid s_0) &amp;=(1-\tilde{\gamma})\sum_{t\geq 0} \tilde{\gamma}^t\tilde{p}^\pi(s_t=s^+ \mid s_0)\\ &amp;=(1-\tilde{\gamma})\sum_{t\geq 0} \tilde{\gamma}^t\tilde{p}^\pi(t_{s^+}\leq t \mid s_0)\\ &amp;=(1-\tilde{\gamma})\sum_{t\geq 0} \tilde{\gamma}^t\sum_{k \leq t}\tilde{p}^\pi(t_{s^+}= k \mid s_0)\\ &amp;= (1-\tilde{\gamma}) \sum_{k \geq 1} \tilde{p}^{\pi}(t_{s^+} = k\mid s_0) \sum_{t\geq k}\tilde{\gamma}^t \\ &amp;=\sum_{k \geq 1} \tilde{\gamma}^k \tilde{p}^{\pi}(t_{s^+} = k\mid s_0) \\ &amp;= (1-\alpha) \,\tilde{\gamma}\, \sum_{k \geq 1} \tilde{\gamma}^{k-1} \sum_{s' \in \mathcal{S}} \alpha^{k-1} p^{\pi}(s_{k-1}=s'\mid s_0)\,r(s')\\ &amp;= (1-\alpha) \,\tilde{\gamma}\, \sum_{s' \in \mathcal{S}} r(s') \sum_{k \geq 0} (\tilde{\gamma}\alpha)^{k} \,p^{\pi}(s_{k}=s'\mid s_0)\\ &amp;= \frac{(1-\alpha) \,\tilde{\gamma} }{1-\alpha \tilde{\gamma}}\,\mathbb{E}_{d^\pi_\gamma(s\mid s_0)}[r(s)] \end{align*}\] <p>Recognizing that the term \(\sum_{k\geq 0} (\alpha\tilde{\gamma})^k p^\pi(s_k=s')\) represents the occupancy measure of the original MDP with an effective discount factor of \(\gamma = \alpha \tilde{\gamma}\), we obtain the final result. Thus, maximizing the original return with discount factor \(\gamma\) is equivalent to maximizing \(\tilde{d}^\pi_{\tilde{\gamma}}(s^+)\) in the constructed GC-MDP, where:</p> <p>Recognizing that the term \(\sum_{k\geq 0} (\alpha\tilde{\gamma})^k p^\pi(s_k=s')\) represents the occupancy measure of the original MDP with an effective discount factor of \(\gamma = \alpha \tilde{\gamma}\), we obtain the final result. Thus, maximizing the original return with discount factor \(\gamma\) is equivalent to maximizing \(\tilde{d}^\pi(s^+)\) in the constructed GC-MDP, where:</p> \[\gamma = \alpha\,\tilde{\gamma}.\] <p>Hence, we have established the equivalence:</p> \[J(\pi) = d^\pi_{\tilde{\gamma}}(s^+)\] <p>where \(J(\pi)\) is the reward maximization objective in the original MDP.</p> <hr> <div style="background-color: #E3F2FD; padding: 15px; border-left: 4px solid #1E88E5; margin: 20px 0;"> <span style="color: #1565C0; font-weight: 600;">üìù Note on Extensions:</span> <ul style="color: #424242; margin-top: 8px;"> <li> <strong>Non-normalized rewards:</strong> For rewards $$r(s) \in [r_{\min}, r_{\max}]$$, apply normalization: $$\hat{r}(s) = \frac{r(s) - r_{\min}}{r_{\max} - r_{\min}}$$ to transform to $$[0,1]$$.</li> <li> <strong>State-action rewards:</strong> For $$r(s,a)$$, marginalize over actions: $$\hat{r}(s) = \mathbb{E}_{a \sim \pi(\cdot|s)}[r(s,a)]$$ to convert to state-only rewards.</li> </ul> </div> <hr> <h1 id="experiments">Experiments</h1> <p>We conduct experiments in the Riverswim environment, which requires exploration to reach the end of the river (linear chain of states) by choosing to move right at each state. The agent receives a reward of 1.0 in the rightmost state and receives a small distractor reward of 0.005 if it moves left at any other state. To avoid policy initialization bias, we randomize which action moves left vs. right at each state.</p> <p>We train SGCRL on an goal-conditioned version of riverswim augmented with a positive and negative absorbing state. We compare SGCRL to a baseline Deep Q-learning (DQN) and a baseline Proximal Policy Optimization (PPO) agent trained in the standard riverswim environment (with explicit rewards). For all training performance curves, we plot the success rate of the policy in the augmented environment - the rate at which the agent reaches the positive absorbing goal state. We also plot the mean augmented success rate for an oracle agent that always takes the optimal policy. We find that for shorter river lengths and shorter horizons, DQN converges faster and achieves higher returns than SGCRL. However, as river length and horizon increase, SGCRL achieves similar or higher returns than DQN (Fig. 1). This results suggest that, while SGCRL is not sample efficient for short-horizon exploration tasks, the GC-MDP formulation enables improved exploration in longer-horizon exploration tasks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/GCMDP-blogpost/assets/img/2026-04-27-GCMDP/riverswim_results-480.webp 480w,/GCMDP-blogpost/assets/img/2026-04-27-GCMDP/riverswim_results-800.webp 800w,/GCMDP-blogpost/assets/img/2026-04-27-GCMDP/riverswim_results-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/GCMDP-blogpost/assets/img/2026-04-27-GCMDP/riverswim_results.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Figure 1.</strong> Success rate of DQN, PPO, SGCRL, and oracle in the augmented GC-MDP. Success rate is averaged over 5 seeds with standard error shown.</p> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/GCMDP-blogpost/assets/bibliography/2026-04-27-GCMDP.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/GCMDP-blogpost/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/GCMDP-blogpost/blog/2026/symbolic-connect/">Symbolism Outside, Connectionism Inside: The Trend of Fusing LLMs and Automatic Programs with Symbolic Intermediate Representations</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/GCMDP-blogpost/blog/2026/sac-massive-sim/">Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/GCMDP-blogpost/blog/2026/pushing-meta-cl-methods/">Pushing Meta-Continual Learning Algorithms to the Limit</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/GCMDP-blogpost/blog/2026/nlp-for-human-sciences/">Language as a Window Into the Mind: How NLP and LLMs Advance Human Sciences</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/GCMDP-blogpost/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/GCMDP-blogpost/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/GCMDP-blogpost/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/GCMDP-blogpost/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/GCMDP-blogpost/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/GCMDP-blogpost/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/GCMDP-blogpost/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/GCMDP-blogpost/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/GCMDP-blogpost/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/GCMDP-blogpost/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/GCMDP-blogpost/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/GCMDP-blogpost/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/GCMDP-blogpost/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/GCMDP-blogpost/assets/js/search-data.js"></script> <script src="/GCMDP-blogpost/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>