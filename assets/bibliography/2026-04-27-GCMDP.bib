@article{silver2018general,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Yang and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@article{amin2025pi,
  title={$\pi^{*}_{0.6}$: a VLA That Learns From Experience},
  author={Amin, Ali and Aniceto, Raichelle and Balakrishna, Ashwin and Black, Kevin and Conley, Ken and Connors, Grace and Darpinian, James and Dhabalia, Karan and DiCarlo, Jared and Driess, Danny and others},
  journal={arXiv preprint arXiv:2511.14759},
  year={2025}
}

@inproceedings{liu2025a,
  title={A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive {RL} without Rewards, Demonstrations, or Subgoals},
  author={Grace Liu and Michael Tang and Benjamin Eysenbach},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=xCkgX4Xfu0}
}

@inproceedings{kaelbling1993learning,
  title={Learning to achieve goals},
  author={Kaelbling, Leslie Pack},
  booktitle={IJCAI},
  volume={2},
  pages={1094--8},
  year={1993}
}


@inproceedings{schaul2015universal,
  title={Universal value function approximators},
  author={Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle={International conference on machine learning},
  pages={1312--1320},
  year={2015},
  organization={PMLR}
}

@inproceedings{eysenbach2022contrastive,
  title={Contrastive Learning as Goal-Conditioned Reinforcement Learning},
  author={Benjamin Eysenbach and Tianjun Zhang and Sergey Levine and Ruslan Salakhutdinov},
  booktitle={Advances in Neural Information Processing Systems},
  editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year={2022},
}

@article{zheng2023contrastive,
  title={Contrastive difference predictive coding},
  author={Zheng, Chongyi and Salakhutdinov, Ruslan and Eysenbach, Benjamin},
  journal={arXiv preprint arXiv:2310.20141},
  year={2023}
}

@misc{bastankhah2025,
  title={Demystifying the Mechanisms Behind Emergent Exploration in Goal-conditioned RL}, 
  author={Mahsa Bastankhah and Grace Liu and Dilip Arumugam and Thomas L. Griffiths and Benjamin Eysenbach},
  year={2025},
  eprint={2510.14129},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
}

@inproceedings{thakoor2022generalised,
  title={Generalised policy improvement with geometric policy composition},
  author={Thakoor, Shantanu and Rowland, Mark and Borsa, Diana and Dabney, Will and Munos, R{\'e}mi and Barreto, Andr{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={21272--21307},
  year={2022},
  organization={PMLR}
}

@inproceedings{ghosh2023reinforcement,
  title={Reinforcement learning from passive data via latent intentions},
  author={Ghosh, Dibya and Bhateja, Chethan Anand and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={11321--11339},
  year={2023},
  organization={PMLR}
}


@inproceedings{frans2024unsupervised,
  title={Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings},
  author={Kevin Frans and Seohong Park and Pieter Abbeel and Sergey Levine},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
}

@article{touati2021learning,
  title={Learning one representation to optimize all rewards},
  author={Touati, Ahmed and Ollivier, Yann},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={13--23},
  year={2021}
}

@article{janner2020gamma,
  title={Gamma-models: Generative temporal difference learning for infinite-horizon prediction},
  author={Janner, Michael and Mordatch, Igor and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1724--1735},
  year={2020}
}

@article{dayan1993improving,
  title={Improving generalization for temporal difference learning: The successor representation},
  author={Dayan, Peter},
  journal={Neural computation},
  volume={5},
  number={4},
  pages={613--624},
  year={1993},
  publisher={MIT Press}
}

@article{barreto2017successor,
  title={Successor features for transfer in reinforcement learning},
  author={Barreto, Andr{\'e} and Dabney, Will and Munos, R{\'e}mi and Hunt, Jonathan J and Schaul, Tom and van Hasselt, Hado P and Silver, David},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@inproceedings{pmlr-v37-schaul15,
  title = 	 {Universal Value Function Approximators},
  author = 	 {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1312--1320},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schaul15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/schaul15.html},
  abstract = 	 {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.}
}
@inproceedings{NIPS2017_453fadbd,
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {Hindsight Experience Replay},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf},
  volume = {30},
  year = {2017}
}

@misc{bertsekasNDP,
  title={Dynamic Programming and Optimal Control},
  author={Bertsekas, Dimitri},
  year={2023},
  howpublished={\url{https://web.mit.edu/dimitrib/www/NDP.pdf}}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={2778--2787},
  year={2017},
  organization={PMLR}
}

@misc{levy2019learningmultilevelhierarchieshindsight,
  title={Learning Multi-Level Hierarchies with Hindsight}, 
  author={Andrew Levy and George Konidaris and Robert Platt and Kate Saenko},
  year={2019},
  eprint={1712.00948},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/1712.00948}, 
}
@inproceedings{10.5555/3327144.3327250,
  author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  title = {Data-efficient hierarchical reinforcement learning},
  year = {2018},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  abstract = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higherand lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher-and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.},
  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages = {3307–3317},
  numpages = {11},
  location = {Montr\'{e}al, Canada},
  series = {NIPS'18}
}

